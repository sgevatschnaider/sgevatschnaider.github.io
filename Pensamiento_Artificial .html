<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje" />
  <title>Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje</title>
  <!-- Fuente desde Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet" />
  <style>
    :root {
      --primary-color: #007BFF;
      --secondary-color: #0056b3;
      --text-color: #333;
      --background-color: #f4f4f4;
      --container-bg: #fff;
      --highlight-bg: #fff59d;
    }
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      font-family: 'Roboto', sans-serif;
      line-height: 1.6;
      color: var(--text-color);
      background-color: var(--background-color);
      padding: 20px;
      transition: background-color 0.3s, color 0.3s;
    }
    .container {
      max-width: 1000px;
      margin: 40px auto;
      background: var(--container-bg);
      padding: 20px 30px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
      transition: background-color 0.3s;
    }
    h1, h2, h3, h4 {
      text-align: center;
      margin-bottom: 16px;
    }
    h1 {
      color: var(--primary-color);
      margin-bottom: 20px;
    }
    h2, h3, h4 {
      color: var(--secondary-color);
      margin-top: 30px;
    }
    p, li {
      margin-bottom: 16px;
      text-align: justify;
    }
    ol, ul {
      margin-left: 20px;
      margin-bottom: 1em;
    }
    a {
      color: var(--primary-color);
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    /* Resaltado de palabras clave */
    strong {
      background-color: var(--highlight-bg);
      padding: 0 3px;
      border-radius: 3px;
      color: #e60000;
    }
    /* Botones y enlaces */
    .link-button, .reference-button, .theme-toggle-button {
      border: none;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.95rem;
      transition: background-color 0.3s ease;
      padding: 8px 12px;
      margin: 8px 4px;
      text-decoration: none;
      display: inline-block;
    }
    .link-button {
      background-color: #28a745;
      color: #fff;
    }
    .link-button:hover {
      background-color: #218838;
    }
    .reference-button {
      background-color: #28a745;
      color: #fff;
      display: block;
      text-align: left;
      width: calc(100% - 16px);
      margin: 10px 5px;
    }
    .reference-button:hover {
      background-color: #218838;
    }
    .theme-toggle-button {
      background-color: var(--secondary-color);
      color: #fff;
      margin: 0 6px;
    }
    .theme-toggle-button:hover {
      background-color: #003d85;
    }
    /* Widget para elegir idioma */
    .language-widget {
      background: var(--container-bg);
      border-radius: 5px;
      padding: 10px 15px;
      margin: 0 auto 20px;
      box-shadow: 0 2px 3px rgba(0,0,0,0.1);
      max-width: 300px;
      text-align: center;
    }
    .language-widget h3 {
      margin-bottom: 0.5em;
      font-size: 1rem;
    }
    .language-widget button {
      background-color: #0056b3;
      color: #fff;
      border: none;
      padding: 8px 12px;
      margin: 4px;
      border-radius: 4px;
      cursor: pointer;
      transition: background-color 0.3s ease;
    }
    .language-widget button:hover {
      background-color: #003d85;
    }
    /* Secciones de idioma */
    #content-es, #content-en {
      display: none;
      animation: fadeIn 0.5s ease-in;
    }
    @keyframes fadeIn {
      from { opacity: 0; }
      to { opacity: 1; }
    }
    /* Estilos para tablas */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1em 0;
    }
    table, th, td {
      border: 1px solid #ddd;
    }
    th, td {
      padding: 8px;
      text-align: center;
    }
    th {
      background-color: var(--primary-color);
      color: #fff;
    }
    /* Modo Oscuro */
    body.dark-mode {
      background-color: #2c3e50;
      color: #ecf0f1;
    }
    body.dark-mode .container {
      background-color: #34495e;
      color: #ecf0f1;
    }
    body.dark-mode h1, body.dark-mode h2, body.dark-mode h3, body.dark-mode h4 {
      color: #ecf0f1;
    }
    body.dark-mode a {
      color: #9bd4ff;
    }
    body.dark-mode strong {
      background-color: #e67e22;
      color: #ffffff;
    }
    .theme-toggle-container {
      text-align: center;
      margin-bottom: 20px;
    }
    .separator {
      text-align: center;
      margin: 20px 0;
    }
    /* Estilo para las secciones de referencias (ocultas por defecto) */
    .references {
      display: none;
      margin: 10px;
      padding: 10px;
      background-color: #f1f1f1;
      border-left: 4px solid var(--primary-color);
    }
    body.dark-mode .references {
      background-color: #3b4a59;
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Widget de selecci√≥n de idioma -->
    <div class="language-widget">
      <h3>üåê Seleccionar idioma / Select Language</h3>
      <button id="btn-es" type="button">Espa√±ol</button>
      <button id="btn-en" type="button">English</button>
    </div>
    <!-- Bot√≥n para cambiar tema -->
    <div class="theme-toggle-container">
      <button class="theme-toggle-button" onclick="toggleTheme()">Cambiar Tema / Toggle Theme</button>
    </div>
    <main>
      <!-- CONTENIDO EN ESPA√ëOL -->
      <div id="content-es" class="lang">
        <!-- Imagen de cabecera -->
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-Uytdizaj-782Vc2GTaVzIW_wQBK2fUL4HGUZDaSIK5loBwx6wqA4z1WOx3X1-WRZzu_V5_aeelr_6KCwcbaYUQUOktvXmF25I47GHcFQ45TkM0pBkCNlQSCGQPc8kID3wT1TDhyphenhypheng3Z4ByzOZa9u_Rp7YE3dP3xtfK6uYvqS7pq2DxXTk7cOIa-_z1Jg/s320/Introducci%C3%B3n.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj-Uytdizaj-782Vc2GTaVzIW_wQBK2fUL4HGUZDaSIK5loBwx6wqA4z1WOx3X1-WRZzu_V5_aeelr_6KCwcbaYUQUOktvXmF25I47GHcFQ45TkM0pBkCNlQSCGQPc8kID3wT1TDhyphenhypheng3Z4ByzOZa9u_Rp7YE3dP3xtfK6uYvqS7pq2DxXTk7cOIa-_z1Jg/s320/Introducci%C3%B3n.png" 
              alt="Introducci√≥n (ES) / Introduction (EN) - Muestra la idea inicial del tema" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <h1>Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje</h1>
        <h2>Abrir la Caja Negra: Grafos de Atribuci√≥n y Razonamiento en Modelos de Lenguaje</h2>
        <h3>√çndice</h3>
        <ol>
          <li>Introducci√≥n</li>
          <li>Marco conceptual: Interpretabilidad mec√°nica</li>
          <li>Del supergrafo al subgrafo: el modelo como red din√°mica</li>
          <li>Construyendo el microscopio: CLTs y modelo de reemplazo</li>
          <li>Visualizando el razonamiento: grafos de atribuci√≥n</li>
          <li>Usando el microscopio: an√°lisis de casos reales</li>
          <li>¬øLos LLMs piensan? Lo que revelan los grafos</li>
          <li>Limitaciones actuales y futuro del campo</li>
          <li>Conclusi√≥n: hacia una ciencia de la IA interpretable</li>
          <li>Definiciones t√©cnicas</li>
          <li>Referencias</li>
        </ol>
        <!-- Secci√≥n: Introducci√≥n -->
        <h3>1. Introducci√≥n</h3>
        <p>
          En los √∫ltimos a√±os, los modelos de lenguaje han demostrado capacidades sorprendentes, desde responder preguntas con precisi√≥n hasta escribir poes√≠a o resolver acertijos. Pero una pregunta clave persiste: ¬øc√≥mo ‚Äúpiensan‚Äù estos modelos? ¬øSon simplemente predictores de la pr√≥xima palabra, o hay mecanismos internos m√°s complejos operando bajo la superficie?
        </p>
        <p>
          Dos trabajos recientes de Anthropic nos ofrecen una nueva lente para explorar esta cuesti√≥n. El primero, <em>"Circuit Tracing"</em>, presenta un enfoque novedoso para abrir la caja negra de los modelos mediante la construcci√≥n de grafos de atribuci√≥n, una herramienta que permite rastrear c√≥mo fluye la informaci√≥n dentro del modelo. El segundo, <em>"On the Biology of a Language Model"</em>, aplica esta herramienta para revelar comportamientos emergentes que van mucho m√°s all√° de la autorregresi√≥n simple.
        </p>
        <p>
          En esta entrada exploraremos ambos trabajos como partes de una misma narrativa: el desarrollo de una metodolog√≠a poderosa y su aplicaci√≥n para descubrir que los modelos de lenguaje, lejos de ser aut√≥matas ciegos, pueden mostrar se√±ales de razonamiento estructurado, planificaci√≥n textual e incluso simulaci√≥n de pensamiento.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGsZZ2olal53Bu12_Zq337vTOWJTQn6XkBhoTHLJLL7a7jCVJen7ZoJuJBjBlK180kYLx5Og2VTO4npRh2R3kr6ABIgN5-ZNHCZgXCjrJfMKkeVyxzojy_IfQ0N8W8bZ2uP0uG7VxoS2FRhZO9WPMRkDrH1o6RsWc_KZuSMofQhnTx2nk65Dg7yN_rCQc/s320/interpretabilidad.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGsZZ2olal53Bu12_Zq337vTOWJTQn6XkBhoTHLJLL7a7jCVJen7ZoJuJBjBlK180kYLx5Og2VTO4npRh2R3kr6ABIgN5-ZNHCZgXCjrJfMKkeVyxzojy_IfQ0N8W8bZ2uP0uG7VxoS2FRhZO9WPMRkDrH1o6RsWc_KZuSMofQhnTx2nk65Dg7yN_rCQc/s320/interpretabilidad.png" 
              alt="Interpretabilidad mec√°nica (ES) / Mechanical interpretability (EN) - Representa el an√°lisis interno de un modelo" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Marco conceptual -->
        <h3>2. Marco conceptual: Interpretabilidad mec√°nica</h3>
        <p>
          Cuando hablamos de interpretar un modelo de lenguaje, solemos pensar en ver qu√© palabras activas, qu√© partes del input son importantes, o c√≥mo responde ante cambios en el prompt. Sin embargo, la interpretabilidad mec√°nica propone ir mucho m√°s all√°: busca entender los mecanismos internos reales que el modelo utiliza para razonar, de forma an√°loga a c√≥mo la neurociencia estudia los circuitos del cerebro.
        </p>
        <p>
          En lugar de tratar al modelo como una caja negra que convierte inputs en outputs, la interpretabilidad mec√°nica lo analiza como un sistema din√°mico compuesto por features, flujos de activaci√≥n y relaciones causales internas. La pregunta ya no es solo ‚Äú¬øqu√© predice?‚Äù, sino: ‚Äú¬øc√≥mo lo calcula?‚Äù.
        </p>
        <p>
          Este enfoque no solo abre una nueva puerta al entendimiento t√©cnico, sino que tambi√©n sienta las bases para construir modelos de IA m√°s transparentes, depurables y confiables.
        </p>
        <!-- Secci√≥n: Del supergrafo al subgrafo -->
        <h3>3. Del supergrafo al subgrafo: el modelo como red din√°mica</h3>
        <p>
          Los modelos de lenguaje modernos, como los transformadores, pueden entenderse como redes computacionales altamente conectadas. En cada capa, cada token puede influir en muchos otros mediante mecanismos de atenci√≥n, mientras que las redes feed-forward (MLPs) mezclan informaci√≥n de manera compleja. Si representamos todas las posibles rutas de activaci√≥n, obtenemos un <strong>supergrafo computacional</strong>: un grafo dirigido y denso, en el que la informaci√≥n fluye a trav√©s de millones de conexiones posibles.
        </p>
        <p>
          Pero en la pr√°ctica, solo una peque√±a parte de ese supergrafo est√° activa para un prompt espec√≠fico. Esa porci√≥n activa es lo que Anthropic llama el <strong>grafo de atribuci√≥n</strong>: un subgrafo que representa las rutas efectivas de c√°lculo que el modelo us√≥ para generar su predicci√≥n. Este subgrafo incluye solo los nodos activos (features, embeddings, logits) y las conexiones lineales relevantes entre ellos.
        </p>
        <p>
          Esta visi√≥n del supergrafo al subgrafo permite analizar el pensamiento del modelo como un flujo estructurado de decisiones, y abre la puerta a intervenir directamente en sus mecanismos internos para ver qu√© pasa si modificamos una ruta espec√≠fica.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiopOWgMU5duqEbGTORMw8d5b-WRlZPOEEEhtcKmptJgb80iYvbUMOO6j04vTQZ4JSzF4sLyux2jgAmJsTxxlhJrM_G74PmvfaJiQF1vC3zmd-Jj8rb-M1W4dwG8kMTHa9IzRr0yPjV5GmH4WsOQCBP9tnU-Nl8T7xCudO-jz8NxYfpr7_G1Rx-oT7z7lQ/s320/supergrafo.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiopOWgMU5duqEbGTORMw8d5b-WRlZPOEEEhtcKmptJgb80iYvbUMOO6j04vTQZ4JSzF4sLyux2jgAmJsTxxlhJrM_G74PmvfaJiQF1vC3zmd-Jj8rb-M1W4dwG8kMTHa9IzRr0yPjV5GmH4WsOQCBP9tnU-Nl8T7xCudO-jz8NxYfpr7_G1Rx-oT7z7lQ/s320/supergrafo.png" 
              alt="Supergrafo computacional y subgrafo de atribuci√≥n (ES) / Computational supergraph and attribution subgraph (EN) - Ilustra la compleja red de rutas de informaci√≥n" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Construyendo el microscopio -->
        <h3>4. Construyendo el microscopio: CLTs y modelo de reemplazo</h3>
        <p>
          Para poder observar con precisi√≥n el flujo interno de informaci√≥n en un modelo de lenguaje, los autores del paper desarrollan una herramienta central: los <strong>Cross-Layer Transcoders (CLTs)</strong>. Estas redes sustituyen las MLPs originales del modelo, que suelen ser dif√≠ciles de interpretar debido a sus neuronas polis√©micas, por un conjunto de features escasas y m√°s comprensibles.
        </p>
        <p>
          Cada CLT se comporta como una unidad de traducci√≥n: toma las activaciones del residual stream, las convierte en una combinaci√≥n lineal escasa de features, y luego escribe hacia m√∫ltiples capas posteriores del modelo. Esto le da m√°s flexibilidad que una MLP tradicional, que solo opera dentro de su capa.
        </p>
        <p>
          Con los CLTs entrenados, los autores construyen un <strong>modelo de reemplazo</strong>: un modelo funcional que utiliza exactamente las mismas capas de atenci√≥n que el original, pero reemplaza todas sus MLPs por estas nuevas unidades interpretables. Aunque este modelo no es id√©ntico al original, logra replicar su comportamiento con alta fidelidad en una buena parte de los casos.
        </p>
        <p>
          Este modelo reemplazado act√∫a como un microscopio computacional, permitiendo observar y rastrear c√≥mo fluyen los efectos de cada feature, algo imposible con las MLPs tradicionales. Adem√°s, permite congelar ciertos componentes del c√≥mputo, como la atenci√≥n y la normalizaci√≥n, facilitando as√≠ el an√°lisis de los caminos de influencia de forma estrictamente lineal.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEit9JfOgOySH_lCTg4ZFHGgQpzsNS4_MGxmAPcEh3IfDxCshMJD57affU5n8ShpRoU2q_zCTXQ-B-Po643sBBhmv8hM15qAPHntSa7Wz_67-7TepKJuoZ7oie2MkaHujMsExrw2DoN1RRY4wYlXJvHlkR2ffZqJJqohxDcuT-pf808xLYibIeWaqnDFTLA/s320/modelo%20de%20reemplazo.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEit9JfOgOySH_lCTg4ZFHGgQpzsNS4_MGxmAPcEh3IfDxCshMJD57affU5n8ShpRoU2q_zCTXQ-B-Po643sBBhmv8hM15qAPHntSa7Wz_67-7TepKJuoZ7oie2MkaHujMsExrw2DoN1RRY4wYlXJvHlkR2ffZqJJqohxDcuT-pf808xLYibIeWaqnDFTLA/s320/modelo%20de%20reemplazo.png" 
              alt="Del modelo original al modelo de reemplazo (ES) / From the original model to the replacement model (EN) - Representa la transici√≥n hacia un modelo m√°s interpretable" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Visualizando el razonamiento -->
        <h3>5. Visualizando el razonamiento: grafos de atribuci√≥n</h3>
        <p>
          Una vez construido el modelo de reemplazo con CLTs, los autores introducen su herramienta m√°s poderosa: el <strong>grafo de atribuci√≥n</strong>. Este grafo es una representaci√≥n visual y cuantitativa del flujo de informaci√≥n que lleva a una predicci√≥n concreta. En √©l, cada nodo representa una unidad activa como un embedding de token, una feature del CLT o un logit de salida, y cada arista representa una contribuci√≥n lineal entre esas unidades.
        </p>
        <p>
          Lo m√°s destacable es que estas contribuciones son lineales por dise√±o: gracias a la estructura del modelo de reemplazo, es posible calcular con precisi√≥n cu√°nto influye una feature sobre otra. Esto convierte al grafo de atribuci√≥n en una especie de circuito explicativo del modelo, donde se puede seguir el ‚Äúcableado interno‚Äù que justifica cada decisi√≥n.
        </p>
        <p>
          Adem√°s, al tratarse de un subgrafo extra√≠do del supergrafo del modelo original, este grafo est√° podado para mostrar solo los caminos relevantes, permitiendo interpretar decisiones complejas como si fueran cadenas causales de activaciones.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3spzDIjjTsV5CYrhCAjGiPemAvRCCPv4C2UgjaUyokWkoc5ebaFo3gTDlKMyyg7LZJ_JWmAchMFWDcKvhviQ80Gmmk0uSzIorJ88NyDxk7ux0TCcIczL7rJb_Mx3Yrf3YkZIroTz0oaQMXCg-9UAU-fjTZyXXITaPbdlDIDFI7H84wXaz98yIf6tHMXo/s320/grafo%20de%20atribuci%C3%B3n.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3spzDIjjTsV5CYrhCAjGiPemAvRCCPv4C2UgjaUyokWkoc5ebaFo3gTDlKMyyg7LZJ_JWmAchMFWDcKvhviQ80Gmmk0uSzIorJ88NyDxk7ux0TCcIczL7rJb_Mx3Yrf3YkZIroTz0oaQMXCg-9UAU-fjTZyXXITaPbdlDIDFI7H84wXaz98yIf6tHMXo/s320/grafo%20de%20atribuci%C3%B3n.png" 
              alt="Grafo de atribuci√≥n (ES) / Attribution graph (EN) - Visualiza c√≥mo fluye la informaci√≥n dentro del modelo" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Usando el microscopio -->
        <h3>6. Usando el microscopio: an√°lisis de casos reales</h3>
        <p>
          Con el modelo de reemplazo y los grafos de atribuci√≥n en mano, los autores aplican su enfoque a casos reales y diversos, demostrando que es posible observar, explicar y validar c√≥mo un modelo de lenguaje toma decisiones en tareas complejas.
        </p>
        <p><strong>Caso 1: Formaci√≥n de acr√≥nimos</strong></p>
        <p>
          Ante un prompt como ‚ÄúThe National Digital Analytics Group (N‚Äù, el modelo predice ‚ÄúDAG)‚Äù. El grafo de atribuci√≥n revela c√≥mo features espec√≠ficas se activan al detectar palabras clave como ‚ÄúDigital‚Äù y ‚ÄúGroup‚Äù, mostrando c√≥mo estas activaciones fluyen para generar el acr√≥nimo.
        </p>
        <p><strong>Caso 2: Memoria factual</strong></p>
        <p>
          Cuando se presenta una frase como ‚ÄúMichael Jordan plays the sport of‚Ä¶‚Äù, el modelo predice ‚Äúbasketball‚Äù. El grafo muestra que primero se activa una feature relacionada con ‚ÄúMichael Jordan‚Äù, la cual a su vez activa otra que asocia al jugador con su deporte caracter√≠stico, representando un razonamiento en dos pasos.
        </p>
        <p><strong>Caso 3: Suma de dos d√≠gitos</strong></p>
        <p>
          En un prompt como ‚Äúcalc: 36 + 59 =‚Äù, el modelo acierta con ‚Äú95‚Äù. El grafo revela que especializadas features (e.g., ‚Äúvalor aproximado‚Äù, ‚Äúd√≠gito final‚Äù o ‚Äúresultado exacto‚Äù) se combinan para producir la respuesta correcta. Este ejemplo ilustra un razonamiento modular y num√©rico emergente.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6gZAysK6lHybvxyXRPvaV6GKOLQ4sRR1y2Pqf-tz6KDN-tcuWOvBdyRLgjGdU1VRg5R96C-WV5BXw96oWSuscMZ2Eyve8ORAYa23R0nXY0W2ph5VS8HHXNsiQ1PZmlB7pj5Pnsa-X6oEEkE3vleykbME1GSH9hzrZo0U9u-5mYr8IxvG7VDVbYzsJM6o/s407/microscopio.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6gZAysK6lHybvxyXRPvaV6GKOLQ4sRR1y2Pqf-tz6KDN-tcuWOvBdyRLgjGdU1VRg5R96C-WV5BXw96oWSuscMZ2Eyve8ORAYa23R0nXY0W2ph5VS8HHXNsiQ1PZmlB7pj5Pnsa-X6oEEkE3vleykbME1GSH9hzrZo0U9u-5mYr8IxvG7VDVbYzsJM6o/s407/microscopio.png" 
              alt="Microscopio (ES) / Microscope (EN) - Permite analizar y rastrear rutas internas de c√≥mputo" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: ¬øLos LLMs piensan? -->
        <h3>7. ¬øLos LLMs piensan? Lo que revelan los grafos</h3>
        <p>
          Uno de los hallazgos m√°s provocadores es que, al observar los grafos de atribuci√≥n, emergen comportamientos que van m√°s all√° de la autorregresi√≥n simple. Los modelos no solo predicen la pr√≥xima palabra en funci√≥n de la anterior, sino que parecen planificar, anticipar y construir internamente ideas estructuradas.
        </p>
        <p>
          Por ejemplo, en tareas po√©ticas ‚Äîcomo en una rima donde la feature asociada a la palabra ‚Äúrabbit‚Äù se activa antes de completar la l√≠nea‚Äî se evidencia que el modelo ajusta otras partes del texto para encajar con la rima elegida. Asimismo, en tareas l√≥gicas o de diagn√≥stico, se observan activaciones intermedias que indican m√∫ltiples hip√≥tesis convergiendo hacia una decisi√≥n final.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK0G-u2-ZauhBt-18kf3MGTqrMv5T3hz6E9d8mN1StYkii7B_K7yEV8j8zBqbS72JFj7NC_WfZM_MBrm-amujcUBx0Ob6bf0tTrKfzDVRWJpL_DSSHz7epMPX6nGlpBir5sLVLI2oEihhrdUOFWPIp_kLba-gShvzXPjGqRNWZh0QJFO-mU0zKrovxQlw/s418/feature.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK0G-u2-ZauhBt-18kf3MGTqrMv5T3hz6E9d8mN1StYkii7B_K7yEV8j8zBqbS72JFj7NC_WfZM_MBrm-amujcUBx0Ob6bf0tTrKfzDVRWJpL_DSSHz7epMPX6nGlpBir5sLVLI2oEihhrdUOFWPIp_kLba-gShvzXPjGqRNWZh0QJFO-mU0zKrovxQlw/s418/feature.png" 
              alt="Feature (ES) / Feature (EN) - Ejemplo de activaciones clave en el razonamiento del modelo" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Limitaciones -->
        <h3>8. Limitaciones actuales y futuro del campo</h3>
        <p>
          Aunque los resultados son prometedores, los autores reconocen limitaciones t√©cnicas y conceptuales:
        </p>
        <ul>
          <li><strong>Limitaci√≥n 1:</strong> Cobertura parcial del modelo, ya que se basa en un modelo de reemplazo.</li>
          <li><strong>Limitaci√≥n 2:</strong> Componentes no modelados, como la din√°mica de atenci√≥n (QK), que es crucial en los transformers.</li>
          <li><strong>Limitaci√≥n 3:</strong> Generalizaci√≥n y escalabilidad, pues entrenar CLTs y construir grafos requiere recursos significativos.</li>
          <li><strong>Limitaci√≥n 4:</strong> Interferencia en relaciones globales al analizar m√∫ltiples prompts.</li>
        </ul>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuPk9R4s0g-EPW0285u1NYadqmvO8Pq9wYcSxpZGRetl5AmMY1anV43E9dVmGeGfqM37rCZu2AseS8n1J8RUUxjj_L1KEA7Lrlzu5n8CJtU9c-_dy472EaiISCs_4XVOJ7nVgZ9HmQLGu6FwduxgCgzs21rIKIJV7T1Kcz76XxbSNc4a-COb6lxu9lCs/s281/limitaciones.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipuPk9R4s0g-EPW0285u1NYadqmvO8Pq9wYcSxpZGRetl5AmMY1anV43E9dVmGeGfqM37rCZu2AseS8n1J8RUUxjj_L1KEA7Lrlzu5n8CJtU9c-_dy472EaiISCs_4XVOJ7nVgZ9HmQLGu6FwduxgCgzs21rIKIJV7T1Kcz76XxbSNc4a-COb6lxu9lCs/s281/limitaciones.png" 
              alt="Limitaciones del modelo (ES) / Model limitations (EN) - Subraya los principales desaf√≠os y vac√≠os actuales" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Secci√≥n: Conclusi√≥n -->
        <h3>9. Conclusi√≥n: hacia una ciencia de la IA interpretable</h3>
        <p>
          Los dos papers de Anthropic marcan un avance clave hacia una IA auditada desde adentro. Con el uso de modelos de reemplazo y grafos de atribuci√≥n, se demuestra que es posible abrir la caja negra de los modelos de lenguaje y observar, casi como con un microscopio, c√≥mo fluye la informaci√≥n y se construyen las predicciones.
        </p>
        <p>
          M√°s all√° de sus capacidades externas, estos estudios revelan que los LLMs desarrollan circuitos internos organizados, activando conceptos intermedios, construyendo planes ling√º√≠sticos y mostrando un comportamiento que se asemeja al razonamiento. Este enfoque tiene valor cient√≠fico, pr√°ctico y √©tico, ya que permite auditar decisiones, descubrir sesgos y construir herramientas de intervenci√≥n para la seguridad de modelos avanzados.
        </p>
        <!-- Secci√≥n: Definiciones t√©cnicas -->
        <h3>10. Definiciones t√©cnicas</h3>
        <table>
          <thead>
            <tr>
              <th>Concepto</th>
              <th>Definici√≥n</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Interpretabilidad mec√°nica</td>
              <td>Estudio del funcionamiento interno de los modelos de IA, identificando c√≥mo y por qu√© producen sus salidas.</td>
            </tr>
            <tr>
              <td>Grafo de atribuci√≥n</td>
              <td>Subgrafo dirigido que muestra el flujo causal de informaci√≥n entre features, tokens y logits en el modelo.</td>
            </tr>
            <tr>
              <td>Feature</td>
              <td>Unidad interpretable del modelo, construida como una combinaci√≥n lineal escasa de neuronas con un patr√≥n claro.</td>
            </tr>
            <tr>
              <td>Cross-Layer Transcoder (CLT)</td>
              <td>Red que reemplaza MLPs y produce features que escriben en m√∫ltiples capas, facilitando la interpretaci√≥n.</td>
            </tr>
            <tr>
              <td>Modelo de reemplazo</td>
              <td>Versi√≥n modificada del modelo original donde se sustituyen las MLPs por CLTs para an√°lisis interpretativo.</td>
            </tr>
            <tr>
              <td>Modelo congelado</td>
              <td>Configuraci√≥n en la que se fijan la atenci√≥n y la normalizaci√≥n para facilitar un an√°lisis lineal.</td>
            </tr>
            <tr>
              <td>Linealidad de atribuci√≥n</td>
              <td>Propiedad que permite representar los efectos entre features como contribuciones aditivas.</td>
            </tr>
            <tr>
              <td>Supergrafo computacional</td>
              <td>Conjunto de todas las rutas de c√≥mputo posibles dentro del modelo para todos los prompts.</td>
            </tr>
            <tr>
              <td>Perturbaci√≥n e intervenci√≥n</td>
              <td>T√©cnicas para modificar o inhibir features y observar cambios que validen relaciones causales.</td>
            </tr>
          </tbody>
        </table>
        <!-- Secci√≥n: Referencias -->
        <h3>11. Referencias</h3>
        <button class="reference-button" onclick="toggleReferences('refs-es')">Mostrar/Ocultar Referencias</button>
        <div id="refs-es" class="references">
          <p>
            <strong>Circuit Tracing:</strong> Revealing Computational Graphs in Language Models<br/>
            <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html" target="_blank">https://transformer-circuits.pub/2025/attribution-graphs/methods.html</a>
          </p>
          <p>
            <strong>On the Biology of a Large Language Model</strong><br/>
            <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html" target="_blank">https://transformer-circuits.pub/2025/attribution-graphs/biology.html</a>
          </p>
        </div>
      </div>
      
      <!-- CONTENIDO EN INGL√âS -->
      <div id="content-en" class="lang">
        <!-- Imagen de cabecera -->
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6bj_UZjR0M2lrPiezQSqKsXsRfm5tttoOmPiPfv4ImQtow68ui5bXKJljU4wMP8lYeiyLg-y7ib-M1q3GHb0Grov37g7J83MDtVyk4-22oD5aO6myka5I-HFX0IOPr35p_buxO7Yc5stxXNaMoRAr8bgCw2rF0WHmGgJXjl525x9l61wxBQJ7CozihN4/s1536/Introduction.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="Imagen introductoria (ES) / Introductory image (EN) - Representa la idea inicial sobre IA interpretable" 
              border="0" 
              height="320" 
              data-original-height="1536" 
              data-original-width="1024" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj6bj_UZjR0M2lrPiezQSqKsXsRfm5tttoOmPiPfv4ImQtow68ui5bXKJljU4wMP8lYeiyLg-y7ib-M1q3GHb0Grov37g7J83MDtVyk4-22oD5aO6myka5I-HFX0IOPr35p_buxO7Yc5stxXNaMoRAr8bgCw2rF0WHmGgJXjl525x9l61wxBQJ7CozihN4/s320/Introduction.png"
            />
          </a>
        </div>
        <p>Visualizing the Foundations of Interpretable AI</p>
        
        <h1>Artificial Thinking: How Language Models Reason</h1>
        <h2>Opening the Black Box: Attribution and Reasoning Graphs in Language Models</h2>
        <h3>Index</h3>
        <ol>
          <li>Introduction</li>
          <li>Conceptual framework: Mechanical interpretability</li>
          <li>From the supergraph to the subgraph: the model as a dynamic network</li>
          <li>Building the Microscope: CLTs and Replacement Model</li>
          <li>Visualizing Reasoning: Attribution Graphs</li>
          <li>Using the Microscope: Real-Case Analysis</li>
          <li>Do LLMs think? What the graphs reveal</li>
          <li>Current and future limitations of the field</li>
          <li>Conclusion: Towards an interpretable AI science</li>
          <li>Technical definitions</li>
          <li>References</li>
        </ol>
        <!-- Section: Introduction -->
        <h3>1. Introduction</h3>
        <p>
          In recent years, language models have demonstrated amazing capabilities, from answering questions accurately to writing poetry to solving puzzles. But a key question remains: how do these models "think"? Are they simply predictors of the next word, or are there more complex internal mechanisms operating beneath the surface?
        </p>
        <p>
          Two recent works by Anthropic offer us a new lens to explore this question. The first, <em>"Circuit Tracing"</em>, presents a novel approach to opening the black box of models by building attribution graphs that track how information flows within the model. The second, <em>"On the Biology of a Language Model"</em>, applies this tool to reveal emergent behaviors far beyond simple autoregression.
        </p>
        <p>
          In this post, we explore both works as parts of the same narrative: the development of a powerful methodology and its application to discover that language models, far from being blind automatons, can show signs of structured reasoning, textual planning, and even thought simulation.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGsZZ2olal53Bu12_Zq337vTOWJTQn6XkBhoTHLJLL7a7jCVJen7ZoJuJBjBlK180kYLx5Og2VTO4npRh2R3kr6ABIgN5-ZNHCZgXCjrJfMKkeVyxzojy_IfQ0N8W8bZ2uP0uG7VxoS2FRhZO9WPMRkDrH1o6RsWc_KZuSMofQhnTx2nk65Dg7yN_rCQc/s320/interpretabilidad.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGsZZ2olal53Bu12_Zq337vTOWJTQn6XkBhoTHLJLL7a7jCVJen7ZoJuJBjBlK180kYLx5Og2VTO4npRh2R3kr6ABIgN5-ZNHCZgXCjrJfMKkeVyxzojy_IfQ0N8W8bZ2uP0uG7VxoS2FRhZO9WPMRkDrH1o6RsWc_KZuSMofQhnTx2nk65Dg7yN_rCQc/s320/interpretabilidad.png" 
              alt="Interpretabilidad mec√°nica (ES) / Mechanical interpretability (EN) - Profundiza en el funcionamiento interno del modelo" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Section: Conceptual framework -->
        <h3>2. Conceptual framework: Mechanical interpretability</h3>
        <p>
          When we talk about interpreting a language model, we usually think of seeing which words are active, which parts of the input are important, or how the model responds to changes in the prompt. However, mechanical interpretability goes much further: it seeks to understand the real internal mechanisms the model uses to reason, much like neuroscience studies the brain‚Äôs circuits.
        </p>
        <p>
          Instead of treating the model as a black box that converts inputs into outputs, mechanical interpretability analyzes it as a dynamic system composed of features, activation flows, and internal causal relationships. The question is no longer just ‚Äúwhat does it predict?‚Äù but ‚Äúhow does it calculate it?‚Äù.
        </p>
        <p>
          This approach not only opens a new door to technical understanding but also lays the groundwork for building more transparent, debuggable, and reliable AI models.
        </p>
        <!-- Section: From the supergraph to the subgraph -->
        <h3>3. From the supergraph to the subgraph: the model as a dynamic network</h3>
        <p>
          Modern language models, such as transformers, can be understood as highly connected computational networks. At each layer, each token can influence many others through attention mechanisms, while feed-forward networks (MLPs) mix information in complex ways. Representing all possible activation paths gives us a <strong>computational supergraph</strong>: a directed, dense graph through which information flows via millions of possible connections.
        </p>
        <p>
          In practice, however, only a small part of that supergraph is active for a specific prompt. This active portion is what Anthropic calls the <strong>attribution graph</strong>: a subgraph that represents the effective computational paths the model used to generate its prediction. It includes only the active nodes (features, embeddings, logits) and the relevant linear connections between them.
        </p>
        <p>
          This view‚Äîfrom the supergraph to the subgraph‚Äîallows us to analyze the model‚Äôs thinking as a structured flow of decisions, opening the door to directly intervening in its internal mechanisms to see what happens if a specific path is modified.
        </p>
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjj9W1qmJScvOG06F10yfiQ1_NPKnGctgBxLoVOS9Qax7e6YGClutu9jnWT9nc07dNMFKOLGYSx8gK3XhWyLxQt5vNG_3MplQ0j8t5bHuIFA0b0SlWOO5i4rk1oJ4owlGq2LH19v7ddpPQnBzxbiUanvx9frpkC_YIw8k-9Gmu4DNuwM8Ppx8RC36Y3iPw/s1536/Interpretable%20AI%20From%20MLP%20Layers%20to%20Computational%20Feature%20Mapping.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="AI interpretable (ES) / Interpretable AI (EN) - Muestra la conexi√≥n desde capas MLP a mapas de caracter√≠sticas" 
              border="0" 
              width="320" 
              data-original-height="1024" 
              data-original-width="1536" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjj9W1qmJScvOG06F10yfiQ1_NPKnGctgBxLoVOS9Qax7e6YGClutu9jnWT9nc07dNMFKOLGYSx8gK3XhWyLxQt5vNG_3MplQ0j8t5bHuIFA0b0SlWOO5i4rk1oJ4owlGq2LH19v7ddpPQnBzxbiUanvx9frpkC_YIw8k-9Gmu4DNuwM8Ppx8RC36Y3iPw/s320/Interpretable%20AI%20From%20MLP%20Layers%20to%20Computational%20Feature%20Mapping.png"
            />
          </a>
        </div>
        <p>Visualizing Interpretability: From Computational Graphs to Attribution Paths</p>
        <!-- Section: Building the Microscope -->
        <h3>4. Building the Microscope: CLTs and Replacement Model</h3>
        <p>
          To accurately observe the internal flow of information in a language model, the authors of the paper develop a central tool: the <strong>Cross-Layer Transcoders (CLTs)</strong>. These networks replace the model‚Äôs original MLPs‚Äîwhich are typically hard to interpret due to their polysemic neurons‚Äîwith a set of sparse and more understandable features.
        </p>
        <p>
          Each CLT acts as a translation unit: it takes activations from the residual stream, converts them into a sparse linear combination of features, and then writes to multiple downstream layers of the model. This provides more flexibility than a traditional MLP, which only operates within its layer.
        </p>
        <p>
          With trained CLTs, the authors build a <strong>replacement model</strong>: a functional model that uses the same attention layers as the original but replaces all its MLPs with these new interpretable units. Although this model isn‚Äôt identical to the original, it replicates its behavior with high fidelity in many cases.
        </p>
        <p>
          This replacement model acts as a computational microscope, allowing us to observe and track how the effects of each feature flow‚Äîsomething impossible with traditional MLPs. Additionally, it allows certain components, such as attention and normalization, to be frozen, thereby facilitating a strictly linear analysis of influence paths.
        </p>
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEif907gwSaJaKxPERJlPodc5oB_LTvKTEs5jOUkD0klPjth3L6emMUVx64UEV_xT6tS_-9O7QMlty2__-Z4QLjFLoglTjWGBT4c4kSgx975O7N6Kt-HNmKGKRtPPI-geiyu9ur1WaTVput7q7xO3H1oOBQfMjelNMUgJ2dvG_SuKZxcuGPHQzeRNBRQw4Y/s1536/CLTs.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="CLTs (ES) / CLTs (EN) - Ejemplo de redes que sustituyen MLPs para mayor interpretabilidad" 
              border="0" 
              width="320" 
              data-original-height="1024" 
              data-original-width="1536" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEif907gwSaJaKxPERJlPodc5oB_LTvKTEs5jOUkD0klPjth3L6emMUVx64UEV_xT6tS_-9O7QMlty2__-Z4QLjFLoglTjWGBT4c4kSgx975O7N6Kt-HNmKGKRtPPI-geiyu9ur1WaTVput7q7xO3H1oOBQfMjelNMUgJ2dvG_SuKZxcuGPHQzeRNBRQw4Y/s320/CLTs.png"
            />
          </a>
        </div>
        <!-- Section: 5. Visualizing Reasoning -->
        <h3>5. Visualizing Reasoning: Attribution Graphs</h3>
        <p>
          Once the replacement model with CLTs is built, the authors introduce their most powerful tool: the <strong>attribution graph</strong>. This graph is a visual and quantitative representation of the flow of information that leads to a specific prediction. Each node represents an active unit‚Äîsuch as a token embedding, a CLT feature, or an output logit‚Äîand each edge represents a linear contribution between these units.
        </p>
        <p>
          The most remarkable aspect is that these contributions are linear by design: thanks to the structure of the replacement model, it is possible to precisely calculate how much one feature influences another. This makes the attribution graph a kind of explanatory circuit of the model, where one can follow the ‚Äúinternal wiring‚Äù that justifies each decision.
        </p>
        <p>
          Moreover, since it is a subgraph extracted from the original model‚Äôs supergraph, this graph is pruned to show only the relevant paths. This allows us to interpret complex decisions as if they were causal chains of activations.
        </p>
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbYk13NBlKnOBFdpVff2EytR0ftL22YCpNfNlLllQYkSZMLqD3apufKQS8_IE0mEqfC2m7DqMovTT3EObkRgPpth77ADJg1jix-gh098hrkPZo_vRDOG17vZXro36uu5SaCrZelUkwP85O7tyO1okGDIyNAI425bSSxkvCvsl7ATpIj9N4uLztPe2LACA/s1024/atribution%20graph.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="Grafo de atribuci√≥n (ES) / Attribution graph (EN) - Demuestra la contribuci√≥n lineal de cada unidad al resultado" 
              border="0" 
              width="320" 
              data-original-height="1024" 
              data-original-width="1024" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbYk13NBlKnOBFdpVff2EytR0ftL22YCpNfNlLllQYkSZMLqD3apufKQS8_IE0mEqfC2m7DqMovTT3EObkRgPpth77ADJg1jix-gh098hrkPZo_vRDOG17vZXro36uu5SaCrZelUkwP85O7tyO1okGDIyNAI425bSSxkvCvsl7ATpIj9N4uLztPe2LACA/s320/atribution%20graph.png"
            />
          </a>
        </div>
        <!-- Section: 6. Using the Microscope -->
        <h3>6. Using the Microscope: Real-Case Analysis</h3>
        <p>
          Armed with the replacement model and the attribution graphs, the authors apply their approach to a variety of real-world cases, demonstrating that it is possible to observe, explain, and validate how a language model makes decisions in complex tasks.
        </p>
        <p><strong>Case 1: Formation of Acronyms</strong></p>
        <p>
          In response to a prompt like ‚ÄúThe National Digital Analytics Group (N‚Äù, the model predicts ‚ÄúDAG)‚Äù. The attribution graph reveals how specific features activate upon detecting keywords like ‚ÄúDigital‚Äù and ‚ÄúGroup,‚Äù showing how these activations flow to produce the acronym.
        </p>
        <p><strong>Case 2: Factual Memory</strong></p>
        <p>
          When presented with a sentence such as ‚ÄúMichael Jordan plays the sport of‚Ä¶,‚Äù the model predicts ‚Äúbasketball.‚Äù The graph shows that a feature related to ‚ÄúMichael Jordan‚Äù is activated first, which in turn activates another feature that associates the player with his characteristic sport‚Äîrepresenting a two-step reasoning process.
        </p>
        <p><strong>Case 3: Summation of Two Digits</strong></p>
        <p>
          For a prompt like ‚Äúcalc: 36 + 59 =‚Äù, the model correctly outputs ‚Äú95.‚Äù The attribution graph reveals that specialized features (e.g., ‚Äúapproximate value,‚Äù ‚Äúfinal digit,‚Äù ‚Äúexact result‚Äù) combine to produce the right answer. This example illustrates an emergent, modular, numerical reasoning process.
        </p>
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjiBsfl7OAYfa5OI_1vFXeHfJrD7P3zFCKc7H6rPNsIIC36X-_b5NdP02diD9Y9b7mWLH3vdXtFLDekl8L8SqLjQy1wXiQ6tMj4gbeyMuXP2dJxM-5uPyxUFAQmsoLL-2JJhjmHAQM-y7e66ODjrxZdvSdWvW-XJLYLB-X_9sh1qXD3sx1m_voOLo9crzA/s1024/Real-World%20Case%20Analysis%20in%20Interpretable%20AI.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="An√°lisis de casos reales (ES) / Real-world case analysis (EN) - Ilustra ejemplos concretos de interpretabilidad" 
              border="0" 
              width="320" 
              data-original-height="1024" 
              data-original-width="1024" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjiBsfl7OAYfa5OI_1vFXeHfJrD7P3zFCKc7H6rPNsIIC36X-_b5NdP02diD9Y9b7mWLH3vdXtFLDekl8L8SqLjQy1wXiQ6tMj4gbeyMuXP2dJxM-5uPyxUFAQmsoLL-2JJhjmHAQM-y7e66ODjrxZdvSdWvW-XJLYLB-X_9sh1qXD3sx1m_voOLo9crzA/s320/Real-World%20Case%20Analysis%20in%20Interpretable%20AI.png"
            />
          </a>
        </div>
        <!-- Section: 7. Do LLMs think? -->
        <h3>7. Do LLMs think? What the graphs reveal</h3>
        <p>
          One of the most provocative findings is that, when looking at attribution graphs, behaviors emerge that go beyond simple autoregression. Models not only predict the next word based on the previous one but seem to plan, anticipate, and internally construct structured ideas.
        </p>
        <p>
          For example, in poetic tasks‚Äîwhere the feature associated with the word ‚Äúrabbit‚Äù activates before completing the line‚Äîthe model later adjusts other parts of the text to fit the chosen rhyme. Similarly, in logical or medical diagnostic tasks, intermediate activations indicate multiple hypotheses converging toward a final decision.
        </p>
        <div class="separator">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK0G-u2-ZauhBt-18kf3MGTqrMv5T3hz6E9d8mN1StYkii7B_K7yEV8j8zBqbS72JFj7NC_WfZM_MBrm-amujcUBx0Ob6bf0tTrKfzDVRWJpL_DSSHz7epMPX6nGlpBir5sLVLI2oEihhrdUOFWPIp_kLba-gShvzXPjGqRNWZh0QJFO-mU0zKrovxQlw/s418/feature.png">
            <img 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgK0G-u2-ZauhBt-18kf3MGTqrMv5T3hz6E9d8mN1StYkii7B_K7yEV8j8zBqbS72JFj7NC_WfZM_MBrm-amujcUBx0Ob6bf0tTrKfzDVRWJpL_DSSHz7epMPX6nGlpBir5sLVLI2oEihhrdUOFWPIp_kLba-gShvzXPjGqRNWZh0QJFO-mU0zKrovxQlw/s418/feature.png" 
              alt="Feature (ES) / Feature (EN) - Ejemplo de activaciones que evidencian planificaci√≥n y razonamiento" 
              width="320" 
              height="320"
            />
          </a>
        </div>
        <!-- Section: 8. Current and future limitations -->
        <h3>8. Current and future limitations of the field</h3>
        <p>
          Although the results are promising, the authors acknowledge several technical and conceptual limitations:
        </p>
        <ul>
          <li><strong>Limitation 1:</strong> Partial coverage of the model, since the analysis is based on a replacement model.</li>
          <li><strong>Limitation 2:</strong> Non-modeled components (such as attention), which are crucial in transformer dynamics.</li>
          <li><strong>Limitation 3:</strong> Generalizability and scalability, as training CLTs and constructing attribution graphs require significant resources.</li>
          <li><strong>Limitation 4:</strong> Interference in global relationships when analyzing multiple prompts.</li>
        </ul>
        <div class="separator" style="clear: both;">
          <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiScJwn49XMiF7AIah_-v4PzSBSV8LZTPKx-GV5gw9bNGnKdAR2nNxKTiGQU2RQjcaGJoBQKgdX8lY6uegwC6HzYR-eJU4ftp_zHnYXzCXGSfG71Cjr9qbrTKlAVFQB__miTlFtZf_hmbccoR2PsRmLGjarU2uQHEXVv7sqFl2I8jhOBTll6mykJBTeUro/s1024/Current%20limitations.png" style="display: block; padding: 1em 0; text-align: center;">
            <img 
              alt="Limitaciones del modelo (ES) / Model limitations (EN) - Destaca obst√°culos t√©cnicos y conceptuales" 
              border="0" 
              width="320" 
              data-original-height="1024" 
              data-original-width="1024" 
              src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiScJwn49XMiF7AIah_-v4PzSBSV8LZTPKx-GV5gw9bNGnKdAR2nNxKTiGQU2RQjcaGJoBQKgdX8lY6uegwC6HzYR-eJU4ftp_zHnYXzCXGSfG71Cjr9qbrTKlAVFQB__miTlFtZf_hmbccoR2PsRmLGjarU2uQHEXVv7sqFl2I8jhOBTll6mykJBTeUro/s320/Current%20limitations.png"
            />
          </a>
        </div>
        <!-- Section: 9. Conclusion -->
        <h3>9. Conclusion: Towards an interpretable AI science</h3>
        <p>
          The two Anthropic papers represent a key step forward toward an AI that can be understood and audited from within. By using replacement models and attribution graphs, it has been shown that it is possible to open the black box of language models and observe, almost like with a microscope, how information flows and predictions are constructed.
        </p>
        <p>
          Beyond external capabilities, these studies reveal that LLMs develop organized internal circuits: they activate intermediate concepts, construct language plans, and display reasoning-like behaviors. This challenges the traditional view that they only ‚Äúpredict the next word.‚Äù
        </p>
        <p>
          This approach has scientific, practical, and ethical value. It allows diagnosing errors, auditing decisions, discovering biases, and building intervention tools‚Äîcritical for the security of advanced models.
        </p>
        <!-- Section: 10. Technical definitions -->
        <h3>10. Technical definitions</h3>
        <table>
          <thead>
            <tr>
              <th>Concept</th>
              <th>Definition</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Mechanical interpretability</td>
              <td>Study of the inner workings of AI models, identifying how and why they produce their outputs.</td>
            </tr>
            <tr>
              <td>Attribution Graph</td>
              <td>A targeted subgraph that shows the causal flow of information between features, tokens, and logits in the model.</td>
            </tr>
            <tr>
              <td>Feature</td>
              <td>An interpretable unit of the model, constructed as a sparse linear combination of neurons with a clear pattern.</td>
            </tr>
            <tr>
              <td>Cross-Layer Transcoder (CLT)</td>
              <td>A network that replaces MLPs and produces features that write in multiple layers, facilitating interpretation.</td>
            </tr>
            <tr>
              <td>Replacement Model</td>
              <td>A modified version of the original model where MLPs are replaced by CLTs for interpretive analysis.</td>
            </tr>
            <tr>
              <td>Frozen model</td>
              <td>A configuration where attention and normalization are fixed to facilitate linear analysis.</td>
            </tr>
            <tr>
              <td>Linearity of attribution</td>
              <td>A property that allows effects between features to be represented as additive contributions.</td>
            </tr>
            <tr>
              <td>Computer supergraph</td>
              <td>The set of all possible computation paths within the model for all possible prompts.</td>
            </tr>
            <tr>
              <td>Disruption and intervention</td>
              <td>Experimental techniques to modify or inhibit features and observe changes that validate causal relationships.</td>
            </tr>
          </tbody>
        </table>
        <!-- Section: 11. References -->
        <h3>11. References</h3>
        <button class="reference-button" onclick="toggleReferences('refs-en')">Show/Hide References</button>
        <div id="refs-en" class="references">
          <p>
            <strong>Circuit Tracing:</strong> Revealing Computational Graphs in Language Models<br/>
            <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html" target="_blank">https://transformer-circuits.pub/2025/attribution-graphs/methods.html</a>
          </p>
          <p>
            <strong>On the Biology of a Large Language Model</strong><br/>
            <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html" target="_blank">https://transformer-circuits.pub/2025/attribution-graphs/biology.html</a>
          </p>
        </div>
      </div>
    </main>
  </div>
  <script>
    // L√≥gica de cambio de idioma
    const btnEs = document.getElementById('btn-es');
    const btnEn = document.getElementById('btn-en');
    const contentEs = document.getElementById('content-es');
    const contentEn = document.getElementById('content-en');

    // Mostrar espa√±ol por defecto
    contentEs.style.display = 'block';
    contentEn.style.display = 'none';
    document.title = 'Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje';
    document.documentElement.setAttribute('lang', 'es');

    btnEs.addEventListener('click', () => {
      contentEs.style.display = 'block';
      contentEn.style.display = 'none';
      document.title = 'Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje';
      document.documentElement.setAttribute('lang', 'es');
    });

    btnEn.addEventListener('click', () => {
      contentEs.style.display = 'none';
      contentEn.style.display = 'block';
      document.title = 'Artificial Thinking: How Language Models Reason';
      document.documentElement.setAttribute('lang', 'en');
    });

    // L√≥gica para cambiar el tema (modo oscuro/claro)
    function toggleTheme() {
      document.body.classList.toggle('dark-mode');
      if (document.body.classList.contains('dark-mode')) {
        localStorage.setItem('theme', 'dark');
      } else {
        localStorage.setItem('theme', 'light');
      }
    }

    // Al cargar la p√°gina, aplicar la preferencia de tema guardada
    window.addEventListener('DOMContentLoaded', () => {
      const storedTheme = localStorage.getItem('theme');
      if (storedTheme === 'dark') {
        document.body.classList.add('dark-mode');
      }
    });

    // Funci√≥n para mostrar/ocultar las referencias
    function toggleReferences(refId) {
      const refs = document.getElementById(refId);
      if (refs.style.display === 'none' || refs.style.display === '') {
        refs.style.display = 'block';
      } else {
        refs.style.display = 'none';
      }
    }
  </script>
</body>
</html>
