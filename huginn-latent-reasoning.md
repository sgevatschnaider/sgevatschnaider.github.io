# Huginn: M√°s All√° de las Palabras con Razonamiento Latente

[üåê Cambiar idioma](https://economiayetica.blogspot.com/2025/04/huginn-razonamiento-latente-root.html)

---

Los modelos de lenguaje actuales alcanzan su rendimiento gracias al uso intensivo de **tokens**: cada paso del razonamiento se traduce en texto. Esta estrategia, conocida como **chain-of-thought** reasoning, mejora los resultados, pero impone un l√≠mite estructural: obliga a que todo pensamiento pase por el filtro del lenguaje.

En este art√≠culo analizamos ‚Äú**Huginn**‚Äù, una arquitectura innovadora basada en **razonamiento latente** **recurrente**, que permite a los modelos procesar internamente m√∫ltiples pasos antes de emitir una palabra. **Huginn** introduce un nuevo eje de escalabilidad: m√°s profundidad de pensamiento, no m√°s **par√°metros**. Una aproximaci√≥n que desaf√≠a el paradigma actual y abre nuevas posibilidades para el futuro de la inteligencia artificial.

---

## Cartograf√≠a del Pensamiento Latente

![Visualizaci√≥n animada del espacio latente](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgzb3e9KG5oBvd1oAHMGyIHAg8Wpe66E1eMmQVeDTA21HBbec38DmWzw-5QSAp9rLgj5gU2THOu-mFzsZ9ZD_7SoUG2j52JYhK-5Q4_ZRqdALPqYd6__l572-CzSN3fD88rRIVcslxX2MeHpdiNzUBku9JdQycPEFLNmsrdBiMBqAFcbwlaFGhKwKviwM/s400/20250426_1022_Futuristic%20Data%20Cosmos_simple_compose_01jss47n5gf1mrhst3t1ctf3nx.gif)
*La imagen representa un video hecho por Sora para representar el **espacio latente**. Si bien Sora no utiliza un **espacio latente** sino un espacio de p√≠xeles para la generaci√≥n de videos, ambos espacios tienen mucho en com√∫n. El espacio de p√≠xeles, como esta imagen, representa el color real de cada punto, mientras que el **espacio latente** representa informaci√≥n abstracta: contornos, texturas, sem√°ntica.*

---

## 1. Introducci√≥n: ¬øQu√© significa ‚Äúrazonar sin palabras‚Äù?

Durante a√±os, el avance en modelos de lenguaje como **GPT** o **ChatGPT** se centr√≥ en ense√±arles a predecir palabras de forma cada vez m√°s precisa. Estos modelos se volvieron expertos en generar texto fluido, responder preguntas y hasta resolver problemas complejos, todo bas√°ndose en una habilidad central: anticipar la pr√≥xima palabra en una secuencia.

Pero en medio de esta evoluci√≥n, una pregunta empez√≥ a tomar forma entre investigadores: ¬ørealmente los modelos piensan mientras escriben? ¬øO simplemente est√°n encadenando palabras que "suenan bien"? Y si pueden pensar... ¬øpor qu√© obligarlos a traducir cada paso de su razonamiento a palabras?

Esta es la idea disruptiva detr√°s del trabajo que vamos a explorar: que los modelos de lenguaje podr√≠an mejorar, y mucho, si no estuvieran obligados a "hablar" mientras razonan. Que quiz√°, como los humanos, podr√≠an tener un pensamiento interno m√°s abstracto, m√°s rico, m√°s flexible, y s√≥lo convertirlo en palabras al final, cuando ya tienen claro qu√© quieren decir.

La limitaci√≥n actual radica en que estos modelos necesitan expresar sus ideas mediante **tokens**, unidades b√°sicas de texto (como palabras, fragmentos o caracteres). Cada vez que dan un paso en un razonamiento, deben emitir un nuevo **token**. Esto no solo consume tiempo y recursos, sino que tambi√©n fuerza al modelo a empobrecer sus pensamientos, traduci√©ndolos a un lenguaje lineal y simplificado.

Y es ah√≠ donde aparece una nueva propuesta: permitir que el modelo razone "en silencio", en lo que los cient√≠ficos llaman **espacio latente**, un espacio matem√°tico donde la informaci√≥n se representa de forma interna, en vectores de alta dimensi√≥n, sin pasar por palabras.

Esta es la hip√≥tesis que abre la puerta a una nueva generaci√≥n de modelos: m√°s inteligentes, m√°s eficientes, y sobre todo, menos dependientes del lenguaje para pensar.

---

## 2. El problema actual: pensar como humanos, pero hablar como robots

Para entender por qu√© esta nueva l√≠nea de investigaci√≥n es tan importante, primero tenemos que mirar c√≥mo funcionan hoy los modelos de lenguaje m√°s avanzados. Aunque parezcan inteligentes y vers√°tiles, hay un detalle t√©cnico que condiciona toda su forma de ‚Äúpensar‚Äù: deben hacerlo a trav√©s de **tokens**.

Un **token** no es m√°s que una unidad b√°sica de texto: puede ser una palabra completa, una s√≠laba, una letra o incluso un signo de puntuaci√≥n. Cada vez que un modelo responde, lo hace generando una secuencia de **tokens**, uno por uno. Y cada uno de esos **tokens** no es solo una palabra al azar: representa una decisi√≥n que el modelo tom√≥, basada en todos los **tokens** anteriores.

Ahora bien, cuando un modelo tiene que resolver un problema complejo, como una suma larga, una pregunta l√≥gica o una explicaci√≥n cient√≠fica, la estrategia m√°s efectiva que se descubri√≥ hasta ahora es el llamado **chain-of-thought**, o cadena de pensamiento. Esta t√©cnica hace que el modelo escriba paso a paso su razonamiento, como si estuviera explicando en voz alta c√≥mo llega a la respuesta.

El problema es que obligar al modelo a expresar mediante palabras cada paso consume mucha memoria, procesamiento y contexto. Es como si una persona resolviera un problema matem√°tico diciendo en voz alta: ‚ÄúAhora multiplico por dos. Ahora resto cinco. Ahora comparo los resultados‚Ä¶‚Äù. Si lo pensamos este esfuerzo no es necesario para elaborar una respuesta, pues el proceso lo hacemos internamente, en silencio.

Lo mismo pasa con estos modelos: cada vez que generan un **token** est√°n desperdiciando parte de su capacidad computacional en expresarse, en lugar de enfocarse en razonar. Adem√°s, al tener que convertir sus ideas en texto, pierden precisi√≥n. No todo lo que se piensa se puede decir con claridad, y menos a√∫n en un formato limitado por una secuencia de palabras.

En otras palabras: los modelos actuales est√°n dise√±ados para hablar bien, pero no necesariamente para pensar bien. Y eso pone un techo a su capacidad.

La gran pregunta que surge es: ¬øy si pudi√©ramos permitirles pensar como lo hacemos los humanos? Es decir, procesar, comparar, imaginar y razonar sin tener que poner todo en palabras.

Ah√≠ es donde entra la idea de razonar en **espacio latente**, sin **tokens** de por medio. Como si el modelo pudiera ‚Äúdarle vueltas‚Äù a un problema internamente, profundizar su an√°lisis, probar caminos alternativos‚Ä¶ y reci√©n despu√©s, cuando ya est√° listo, generar la respuesta final.

---

### Mapa latente de tokens en el paso inicial del razonamiento

![Mapa latente inicial de tokens](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6Zvy0VfMYA5gOzklGEpQtmSsgxHIYZZAL3yBHYjHAXFhm3_wMn9C0cBOKxx6Yv8utq2_rNmkK53V8jZBGzSusA41SYmUMARF48MIitKUFQ5b7EWtRF0mM9uuPTXQiwc4yOcAZggQraytcEbcJJN6uFgBADWSVQqz3LsXrahxto1l10jEVkKzmlNUb95U/s400/waterfall_mejorado.gif)
*Fuente: Elaboraci√≥n propia en base al repositorio de Scaling up Test-Time Compute with Latent Reasoning*

La imagen anterior representa una visualizaci√≥n del proceso de **razonamiento latente** **recurrente** de un modelo de lenguaje avanzado. Cada punto corresponde al estado interno de un **token** de la entrada en el primer paso de razonamiento. Los ejes horizontales (PCA1 y PCA2) representan una proyecci√≥n reducida del **espacio latente** de alta dimensi√≥n mediante An√°lisis de Componentes Principales (**PCA**), mientras que el eje vertical indica la posici√≥n del **token** en la secuencia original de texto.

Esta imagen es el paso 0 de una animaci√≥n completa (capturada en el GIF asociado), que muestra c√≥mo el estado interno de cada **token** evoluciona con cada iteraci√≥n del bloque **recurrente** del modelo. A lo largo de esta secuencia, el modelo aplica m√∫ltiples veces una funci√≥n de razonamiento sobre su representaci√≥n latente, permiti√©ndole refinar su comprensi√≥n antes de producir una salida verbal.

En el instante inicial, los estados est√°n agrupados verticalmente, lo que refleja que a√∫n no ha ocurrido razonamiento interno: todos los **tokens** han sido apenas embebidos en el **espacio latente**, pero no han interactuado ni refinado su significado.

Sin embargo, al avanzar en las iteraciones (como se ve en la animaci√≥n), cada **token** comienza a trazar una trayectoria √∫nica en el **espacio latente**. Algunas de estas trayectorias:

- Se extienden ampliamente, indicando **tokens** que requieren m√°s pasos de razonamiento (por ejemplo, conceptos clave como ‚Äúdozen‚Äù o ‚Äúweeks‚Äù).
- Permanecen cercanas a su punto inicial, reflejando **tokens** triviales o funcionales como preposiciones o determinantes.
- Forman bucles, espirales o l√≠neas rectas, evidenciando patrones de razonamiento interno como √≥rbitas (iteraci√≥n), sliders (conteo progresivo), o convergencia a punto fijo (comprensi√≥n estabilizada).

Esta visualizaci√≥n revela un aspecto cr√≠tico del nuevo paradigma de razonamiento en modelos de lenguaje: el pensamiento ocurre en silencio, en el **espacio latente**, sin necesidad de cadenas de palabras intermedias. En lugar de razonar expl√≠citamente con texto como en los enfoques cl√°sicos de **chain-of-thought**, el modelo refina internamente su entendimiento antes de emitir una respuesta. Cada paso transforma los vectores latentes de los **tokens**, permitiendo razonamiento progresivo sin verbalizaci√≥n.

---

## 3. El modelo **Huginn**: una mente que razona en silencio

El modelo est√° dise√±ado para pensar profundamente sin necesariamente expresar cada paso en palabras.

La clave est√° en una idea sencilla pero poderosa: reutilizar sus propias capas internas para razonar m√°s tiempo, sin generar texto hasta el final. Es lo que los autores llaman **profundidad recurrente**.

El modelo recibe una entrada, la transforma en un vector interno (como una especie de pensamiento abstracto), y luego repite el proceso de razonamiento sobre ese vector tantas veces como necesite. Reci√©n cuando siente que lleg√≥ a una conclusi√≥n, traduce ese vector a texto.

Esto tiene dos grandes consecuencias:

1.  El modelo puede razonar m√°s profundo con el mismo tama√±o. Aunque **Huginn** tiene 3.5 mil millones de **par√°metros** (una cantidad modesta comparada con los grandes modelos actuales), puede simular ser mucho m√°s profundo simplemente repitiendo sus pasos de razonamiento. En t√©rminos computacionales, esto es como tener un modelo de 50 mil millones de **par√°metros**... sin tener que entrenar uno.
2.  El razonamiento se mantiene en el **espacio latente**, es decir, en un formato interno que no necesita ser traducido a palabras a cada paso. Es como si el modelo pudiera pensar en voz baja, de manera abstracta, sin perder tiempo ni precisi√≥n al convertir cada pensamiento en lenguaje.

Esto representa un cambio de paradigma: pasamos de modelos que generan m√°s **tokens** para pensar mejor, a modelos que usan m√°s computaci√≥n interna sin necesidad de hablar m√°s.

---

## 4. ¬øC√≥mo se entrena un modelo que piensa sin hablar?

En los modelos cl√°sicos, el proceso de entrenamiento consiste b√°sicamente en mostrarle millones de fragmentos de texto y pedirle que prediga el siguiente **token**. Cada vez que se equivoca, ajusta un poco sus **par√°metros**. Este ciclo se repite billones de veces.

Pero con **Huginn** el juego cambia: ahora hay que ense√±arle no solo a predecir palabras, sino a usar sus propias capas internas como una herramienta de razonamiento iterativo. Es como ense√±arle no solo a hablar, sino a reflexionar antes de hablar.

La novedad clave es que **Huginn** tiene un bloque **recurrente**: un conjunto de capas que puede aplicar una y otra vez. Pero para que esto funcione, hay que entrenarlo con distintos ‚Äúritmos de pensamiento‚Äù. Es decir, durante el entrenamiento se le ense√±a a resolver tareas haciendo desde 1 hasta 64 repeticiones internas de ese bloque.

Cada secuencia de texto que se le muestra durante el entrenamiento pasa por un n√∫mero distinto de iteraciones, elegido al azar. As√≠, el modelo aprende a mejorar sus respuestas cuando se le da m√°s tiempo para pensar, y tambi√©n a resolver cosas r√°pido cuando no hace falta tanto procesamiento.

Esto se llama entrenamiento con recurrencia variable, y es lo que le permite a **Huginn** adaptarse din√°micamente durante la inferencia (cuando ya est√° en uso).

Entrenar un modelo **recurrente** no es f√°cil. Repetir muchas veces una misma operaci√≥n puede hacer que la informaci√≥n colapse o explote: o se pierde el sentido (todo se aplana), o se vuelve ca√≥tico.

Para evitar eso, los investigadores usaron varias t√©cnicas inteligentes:

- Inyecci√≥n de estado aleatorio inicial: cada secuencia comienza con un vector interno distinto, lo que ayuda a evitar que el modelo caiga en patrones r√≠gidos.
- Retroalimentaci√≥n controlada: durante el entrenamiento, no se calculan los errores de todas las iteraciones, sino solo de las √∫ltimas. Esto reduce el consumo de memoria y evita que el modelo se abrume intentando ajustar cada paso interno.
- Normas espec√≠ficas: se aplican capas de normalizaci√≥n (como **RMSNorm**) cuidadosamente ubicadas para mantener la estabilidad num√©rica del modelo mientras razona.
- Entrenamiento continuo y modular: el modelo se estructura en tres bloques (preludio, **recurrente**, coda) que pueden ser afinados independientemente.

Todo esto se entrena en un superordenador con miles de **GPUs**, en tandas de hasta 12 horas por segmento. El resultado: un modelo con solo 3.5 mil millones de **par√°metros** que aprendi√≥ a comportarse como si tuviera muchos m√°s, simplemente porque puede ‚Äúpensar m√°s hondo‚Äù.

---

## 5. Resultados sorprendentes: m√°s chico, m√°s eficiente, m√°s inteligente

**Huginn** fue entrenado con 3.5 mil millones de **par√°metros**. Para ponerlo en contexto, es un tama√±o intermedio, bastante m√°s chico que los grandes modelos comerciales como **GPT**-3 (175B) o **PaLM** (540B), e incluso m√°s chico que muchos modelos open source modernos como **Mistral** (7B) o **Mixtral** (12.9B).

Y, sin embargo, **Huginn** logra un rendimiento comparable ‚Äîy a veces superior‚Äî en varias tareas que exigen razonamiento complejo.

¬øC√≥mo? Gracias a su capacidad para escalar razonamiento, no tama√±o. Es decir, puede repetir internamente sus propios procesos de pensamiento, en vez de depender de m√°s capas fijas o m√°s **tokens** de entrenamiento.

En las pruebas de evaluaci√≥n m√°s comunes para modelos de lenguaje, **Huginn** mostr√≥ mejoras progresivas a medida que se le permit√≠a usar m√°s iteraciones internas en su bloque **recurrente** (es decir, pensar m√°s).

Lo m√°s impresionante es que el rendimiento mejora de forma continua con m√°s recursividad, lo cual demuestra que su arquitectura **recurrente** no es un truco puntual, sino una v√≠a robusta para escalar inteligencia.

---

## 6. **Comportamientos emergentes**: cuando el modelo aprende a pensar por su cuenta

### C√≥mo piensa **Huginn**: tres estilos de razonamiento interno

**Huginn** no solo resuelve tareas. Aprende a razonar de formas distintas seg√∫n el desaf√≠o. Esta infograf√≠a resume los tres patrones emergentes que los investigadores identificaron:

- Convergencia para tareas simples.
- √ìrbitas para manipular conceptos abstractos o espaciales.
- Sliders para evaluaciones complejas o morales.

### Patrones emergentes de pensamiento latente en IA

![Patrones emergentes de pensamiento](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjePjtjmPlCTvLGvmjBfyVs9sRsYemuo2uPQJ_TLNLCfM7AuqZDIv5oG9WKbymDDH4_h0emq498r0FmNtbq0KcR5r_6bBzGCIGzxf-_0QnIgUJHAKGvROZH4G7GrYGHhK_ctdqnLA_7pBaTojbpRBVn4SppK9WNlTl2VOVsibl4c_d5bHI5QJC93wg1NmU/s400/trayectorias_mejorado.gif)

Uno de los descubrimientos m√°s fascinantes de este modelo es que no todos los pensamientos siguen el mismo camino interno. Dependiendo de la tarea, **Huginn** razona de formas diferentes y, eso puede verse directamente en su **espacio latente**.

La siguiente imagen muestra c√≥mo se transforma internamente el estado de ciertos **tokens** a medida que el modelo razona en silencio. Cada trayectoria corresponde a una palabra espec√≠fica mientras pasa por m√∫ltiples iteraciones del bloque **recurrente**. Lo notable es que el modelo adopta diferentes patrones de razonamiento espont√°neamente, sin que nadie se lo haya ense√±ado:

- **Convergencia r√°pida:** algunas palabras se estabilizan enseguida, como si el modelo ‚Äúsupiera‚Äù que no necesitan mayor reflexi√≥n.
- **√ìrbitas latentes:** en tareas espaciales o num√©ricas, los vectores internos giran en patrones circulares, como si **Huginn** estuviera manipulando ideas complejas en su mente.
- **Sliders internos:** para razonamientos abstractos, morales o ambiguos, el modelo avanza en una direcci√≥n √∫nica y progresiva, como ajustando una escala interna de valoraci√≥n.

Estos **comportamientos emergentes** apuntan a una idea poderosa: **Huginn** no solo resuelve tareas, aprende a gestionar su propia forma de pensar. Esto lo acerca a un tipo de razonamiento m√°s flexible, adaptativo y, en cierto sentido, estrat√©gico.

---

### Trayectorias latentes de tokens triviales durante el razonamiento **recurrente**

![Visualizaci√≥n de trayectorias latentes para tokens triviales](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-i1Jk_LmN-oPq_RsT-uVw_XyZ-aBc_DeFg-hIj_KlMn-oPq_RsT-uVw_XyZ-aBc_DeFg-hIj/s1600/trivial_token_trajectories.png)
*Fuente: Elaboraci√≥n propia en base al repositorio de Scaling up Test-Time Compute with Latent Reasoning*

La visualizaci√≥n anterior muestra la evoluci√≥n del estado latente de **tokens** seleccionados a lo largo del proceso de razonamiento interno del modelo. Cada fila corresponde a un **token** espec√≠fico del texto (por ejemplo, ‚Äú3‚Äù, ‚Äúmany‚Äù, ‚Äú?‚Äù), y cada columna muestra su trayectoria proyectada en diferentes planos del **espacio latente** reducidos por an√°lisis de componentes principales (**PCA**).

Cada subgr√°fico representa dos componentes principales del **espacio latente**. En ellos, cada punto de color corresponde al estado latente del **token** en una iteraci√≥n del razonamiento. La escala de color indica el momento temporal dentro del proceso, donde los colores m√°s oscuros representan las primeras iteraciones y los m√°s claros las √∫ltimas. La cruz roja se√±ala el centro de masa de la trayectoria, funcionando como referencia para observar el desplazamiento del **token**.

En esta imagen particular, todos los **tokens** muestran trayectorias extremadamente cortas, pr√°cticamente reducidas a un solo punto. Esto indica que no hubo razonamiento significativo para estos **tokens**. El estado latente de cada uno no cambi√≥ de manera sustancial a lo largo de las iteraciones, o bien lo hizo de forma imperceptible. En t√©rminos cognitivos, el modelo identific√≥ r√°pidamente que estos **tokens** no requer√≠an reflexi√≥n adicional.

Esto se explica por la naturaleza de los **tokens** seleccionados, que incluyen signos de puntuaci√≥n (como ‚Äú,‚Äù o ‚Äú.‚Äù), palabras funcionales (‚Äúmany‚Äù), n√∫meros triviales (‚Äú3‚Äù) y s√≠mbolos estructurales como el signo de interrogaci√≥n (‚Äú?‚Äù). Estos elementos tienden a tener una sem√°ntica fija o altamente predecible, por lo que el modelo los comprende correctamente desde la etapa de embedding inicial. Como resultado, no es necesario "pensar m√°s" sobre ellos. El **razonamiento latente** se detiene r√°pidamente y la trayectoria converge en las primeras iteraciones.

Este comportamiento ejemplifica claramente c√≥mo el modelo asigna m√°s recursos computacionales a los elementos que lo requieren, y evita gastar procesamiento en los que ya entiende con certeza. Se trata de una forma de razonamiento adaptativo, donde la reflexi√≥n se distribuye de manera eficiente, simulando un tipo de atenci√≥n cognitiva interna.

---

## 7. Conclusi√≥n: el futuro de la inteligencia artificial podr√≠a no ser tan hablador

Este modelo no se destaca por su tama√±o ni por su cantidad de datos, sino por su capacidad de razonar en silencio, profundizar cuando lo necesita, y mantenerse eficiente cuando no. Como los humanos, administra su esfuerzo cognitivo seg√∫n el problema. A veces resuelve r√°pido. Otras, se toma su tiempo.

Es una visi√≥n m√°s sobria, m√°s eficiente y, tal vez, m√°s cercana a c√≥mo funciona el pensamiento real, tanto en nosotros como en las m√°quinas que estamos construyendo.

---

## Referencias

- [Paper arXiv [Geiping et al., 2025]](https://arxiv.org/abs/2502.05171)
- [Art√≠culo Quanta Magazine [Ananthaswamy, 2025]](https://www.quantamagazine.org/to-make-language-models-work-better-researchers-sidestep-language-20250414)
- [Repositorio HuggingFace Papers](https://huggingface.co/papers/2502.05171)
- [Notebook GitHub [An√°lisis Razonamiento Latente]](https://github.com/sgevatschnaider/sgevatschnaider.github.io/blob/847535e8c021d1ad9488bfca7e33d0d72a2e0c70/notbooks/Huginn_Latent_Reasoning_Analysis_descarga__github_.ipynb)
