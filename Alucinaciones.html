<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="description" content="Alucinaciones y LLMs"/>
  <title>Alucinaciones y LLMs</title>

  <!-- Fuente Roboto -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">

  <style>
    /*
      VARIABLES DE COLOR
      Ajusta los valores hexadecimales a tu gusto.
    */
    :root {
      --primary-color: #007BFF;       /* Azul principal */
      --secondary-color: #0056b3;     /* Azul m√°s oscuro */
      --text-color: #333;             /* Texto base */
      --background-color: #f5f5f5;    /* Fondo general */
      --border-color: #ccc;           /* Bordes suaves */
      --highlight-bg: #fff59d;        /* Color de resaltado para palabras clave */
    }

    /* ESTILOS GENERALES */
    * {
      box-sizing: border-box;
    }

    body {
      font-family: 'Roboto', Arial, sans-serif;
      line-height: 1.6;
      color: var(--text-color);
      margin: 0;
      padding: 20px;
      background-color: var(--background-color);
    }

    h1, h2, h3 {
      color: var(--primary-color);
      margin-top: 1em;
      margin-bottom: 0.6em;
    }

    p {
      margin-bottom: 1em;
    }

    /* Clase para resaltar palabras clave (opcional) */
    .highlight {
      background-color: var(--highlight-bg);
      font-weight: bold;
      padding: 0 4px;
      border-radius: 3px;
    }

    /* Separadores (para im√°genes u otros elementos) */
    .separator {
      text-align: center;
      margin: 20px 0;
      clear: both;
    }

    /*
      BOTONES COLLAPSIBLE
    */
    .collapsible {
      background-color: var(--primary-color);
      color: #fff;
      cursor: pointer;
      padding: 12px 15px;
      border: none;
      border-radius: 5px;
      font-size: 1rem;
      text-align: left;
      width: 100%;
      transition: background-color 0.3s;
      margin-top: 15px;
      outline: none;
      box-shadow: 0 2px 3px rgba(0,0,0,0.15);
      position: relative;
    }
    .collapsible:hover {
      background-color: var(--secondary-color);
    }
    .collapsible::after {
      content: "‚ñº";
      position: absolute;
      right: 15px;
    }
    .collapsible.active::after {
      content: "‚ñ≤";
    }

    /*
      CONTENIDO PLEGABLE
    */
    .content {
      overflow: hidden;
      max-height: 0;
      transition: max-height 0.3s ease;
      margin-top: 5px;
      padding-left: 15px;
      border-left: 3px solid var(--primary-color);
      background-color: #fff;
      box-shadow: inset 0 2px 3px rgba(0,0,0,0.05);
      border-radius: 5px;
      padding-right: 15px;
    }
    .collapsible.active + .content {
      max-height: 2000px; /* Suficientemente grande para mostrar todo el contenido */
      margin-bottom: 1em;
    }

    /*
      WIDGET DE IDIOMA
    */
    .language-widget {
      margin-bottom: 20px;
      text-align: center;
    }
    .language-widget h3 {
      margin-bottom: 10px;
    }
    .language-widget ul {
      list-style: none;
      margin: 0;
      padding: 0;
      display: flex;
      justify-content: center;
      gap: 10px;
    }
    .language-widget li {
      margin: 0;
    }
    .language-widget a {
      display: inline-block;
      padding: 8px 12px;
      background-color: var(--primary-color);
      color: #fff;
      text-decoration: none;
      border-radius: 4px;
      transition: background-color 0.3s;
    }
    .language-widget a:hover {
      background-color: var(--secondary-color);
    }

    /*
      BOTONES DE ENLACE (Referencias)
    */
    .link-buttons {
      margin-top: 30px;
    }
    .link-buttons a {
      display: inline-block;
      margin-right: 10px;
      padding: 10px 15px;
      background-color: var(--primary-color);
      color: #fff;
      text-decoration: none;
      border-radius: 4px;
      transition: background-color 0.3s, transform 0.3s;
    }
    .link-buttons a:hover {
      background-color: var(--secondary-color);
      transform: scale(1.05);
    }
  </style>
</head>
<body>

  <!-- GIF de Encabezado -->
  <div class="separator" style="clear: both;">
    <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJd_0ujI0ECr2I-r5cSAe1sbpqsGjAZeMeQTRUqFC7_kaCpnMp8bfjX47AAHTHSMbB7hHLS01ccBAVgSKhj28o9s8ZE5lceOB51IGQgKsgwqvx4a1W3GBGQsxy4lCooQWCqv2BMvWzXKcFzbUAt8kj0va9ggUs6XYQZZ8dTxX38b1sbw1Q9Hun5eCecfE/s320/20250113_1958_Mystical%20Data%20Labyrinths_simple_compose_01jhgy86t8eph9yne675yf4c1t.gif"
       style="display: block; padding: 1em 0; text-align: center;"
       target="_blank" rel="noopener">
      <img alt="Gif de Ejemplo" border="0" width="320"
           src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjJd_0ujI0ECr2I-r5cSAe1sbpqsGjAZeMeQTRUqFC7_kaCpnMp8bfjX47AAHTHSMbB7hHLS01ccBAVgSKhj28o9s8ZE5lceOB51IGQgKsgwqvx4a1W3GBGQsxy4lCooQWCqv2BMvWzXKcFzbUAt8kj0va9ggUs6XYQZZ8dTxX38b1sbw1Q9Hun5eCecfE/s320/20250113_1958_Mystical%20Data%20Labyrinths_simple_compose_01jhgy86t8eph9yne675yf4c1t.gif"/>
    </a>
  </div>

  <!-- WIDGET PARA CAMBIAR DE IDIOMA -->
  <div class="language-widget">
    <h3>üåê Cambiar Idioma</h3>
    <ul>
      <li>
        <!-- Enlace a la versi√≥n en espa√±ol -->
        <a href="https://economiayetica.blogspot.com/2025/01/alucinaciones-y-llms-variables-de-color.html" rel="noopener noreferrer">
          üá™üá∏ Espa√±ol
        </a>
      </li>
      <li>
        <!-- Enlace a la versi√≥n en ingl√©s -->
        <a href="https://economiayetica.blogspot.com/2025/01/hallucinations-and-llms-color-variables.html" rel="noopener noreferrer">
          üá¨üáß English
        </a>
      </li>
    </ul>
  </div>

  <!-- TITULAR PRINCIPAL -->
  <h1>Alucinaciones y LLMs</h1>

  <!-- SECCI√ìN 1 -->
  <button type="button" class="collapsible">1) Alucinaciones y LLMs</button>
  <div class="content">
    <p>
      Un problema de los <span class="highlight">Modelos de Lenguaje Grande (LLMs)</span> son las alucinaciones.
      Estas se refieren a la generaci√≥n de contenido falso, fabricado o inconsistente con los datos
      con los que se ha entrenado el modelo. Todos los que utilizamos estos sistemas hemos observado
      c√≥mo puede generar afirmaciones que suenan correctas, pero no tienen sustento.
    </p>
  </div>

  <!-- SECCI√ìN 2 -->
  <button type="button" class="collapsible">2) Mitigaci√≥n de las Alucinaciones</button>
  <div class="content">
    <p>
      Para afrontar el problema de las alucinaciones se han implementado diversas t√©cnicas como:
    </p>
    <ul>
      <li><strong>Chain-of-Thought Prompting (Cadena de razonamiento)</strong>. Esta t√©cnica gu√≠a al modelo para que haga expl√≠cito su razonamiento paso a paso antes de llegar a una respuesta.</li>
      <li><strong>Self-Consistency (Autoconsistencia)</strong>. Genera m√∫ltiples respuestas a una misma pregunta utilizando cadenas de razonamiento (Chain-of-Thought) y selecciona la respuesta m√°s com√∫n o consistente entre las generadas.</li>
      <li><strong>Retrieval-Augmented Generation (RAG)</strong>. Combina el modelo de lenguaje con un sistema de recuperaci√≥n de informaci√≥n. Antes de generar una respuesta, el modelo busca datos relevantes en una base de conocimiento externa.</li>
    </ul>
    <p>
      Si bien estas t√©cnicas reducen el nivel de las alucinaciones, el problema persiste.
      La pregunta es si es posible eliminar totalmente las alucinaciones, lo cual lleva al trabajo de investigaci√≥n
      <em>‚ÄúLLMs Will Always Hallucinate, and We Need to Live With This‚Äù</em> donde se da una respuesta negativa,
      considerando a las alucinaciones un elemento estructural de estos sistemas.
    </p>

    <!-- IMAGEN 1 -->
    <div class="separator">
      <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfJ9TndI5upzwgBx4q7sB0TJHfPI-TbqS8k8bjKLYuLvGDqchuii1JX0EmJHA4wWek1SsBunO4sykx52q-daEXsbpxm77utE9iEdNTP1CrAR1eVbgOHUsjHFsXOlfhzuicx4qcroT4Mc0LkZmzLPvshPbd0F53qsA6Gv7hfNxxhuvvXk7HchhmeVJF0NU/s1384/mitigaci%C3%B3n%20alucinaciones.png"
         style="display: block; padding: 1em 0; text-align: center;"
         target="_blank" rel="noopener">
        <img alt="Mitigaci√≥n de Alucinaciones" border="0" width="320"
             src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhfJ9TndI5upzwgBx4q7sB0TJHfPI-TbqS8k8bjKLYuLvGDqchuii1JX0EmJHA4wWek1SsBunO4sykx52q-daEXsbpxm77utE9iEdNTP1CrAR1eVbgOHUsjHFsXOlfhzuicx4qcroT4Mc0LkZmzLPvshPbd0F53qsA6Gv7hfNxxhuvvXk7HchhmeVJF0NU/s320/mitigaci%C3%B3n%20alucinaciones.png"/>
      </a>
    </div>
  </div>

  <!-- SECCI√ìN 3 -->
  <button type="button" class="collapsible">3) Alucinaciones como un problema estructural</button>
  <div class="content">
    <p>
      En ‚Äú<em>LLMs Will Always Hallucinate, and We Need to Live With This</em>‚Äù se busca establecer que las alucinaciones
      en los modelos de lenguaje grande (<span class="highlight">LLMs</span>) no son errores espor√°dicos,
      sino una caracter√≠stica inevitable debido a las limitaciones matem√°ticas y l√≥gicas inherentes de estos sistemas.
    </p>
    <p>
      <strong>¬øCu√°les son estas limitaciones matem√°ticas y l√≥gicas?</strong><br/>
      Estos investigadores utilizan la Teor√≠a de la Computaci√≥n, el Teorema de Incompletitud de G√∂del
      y problemas indecidibles como el Halting Problem para argumentar que ninguna mejora en datos
      o en mecanismos de verificaci√≥n puede eliminar completamente estas alucinaciones.
    </p>
    <p>
      Estas ‚Äúalucinaciones estructurales‚Äù, como lo define este paper, no se deben a errores en los datos de entrenamiento
      o fallos en la implementaci√≥n: son intr√≠nsecas al sistema y no pueden ser eliminadas completamente.
    </p>
  </div>

  <!-- SECCI√ìN 4 -->
  <button type="button" class="collapsible">4) G√∂del y las alucinaciones de las LLMs</button>
  <div class="content">
    <p>
      El Teorema de Incompletitud de G√∂del establece que en cualquier sistema formal suficientemente poderoso
      (como los modelos matem√°ticos que subyacen a los LLMs) se presentan proposiciones que no pueden
      ser demostradas ni refutadas dentro del sistema.
    </p>
    <p>
      Los modelos de lenguaje grande funcionan como sistemas formales: procesan datos para generar salidas
      basadas en reglas l√≥gicas y probabil√≠sticas. Sin embargo, debido a su naturaleza, comparten las limitaciones
      se√±aladas por G√∂del:
    </p>
    <p>
      La prueba del texto se basa en lo siguiente:<br/>
      <em>Suposici√≥n inicial:</em> Supongamos que existe un conjunto de datos de entrenamiento D que contiene
      todos los hechos verdaderos posibles. Se introduce al conjunto una proposici√≥n A que dice:<br/>
      ‚ÄúExisten hechos verdaderos que no est√°n en D‚Äù.
    </p>
    <p>
      En ese caso, si esta afirmaci√≥n es falsa, no existen hechos verdaderos fuera de D.
      Sin embargo, esto implica que D es falsa porque incluye A.<br/>
      En el caso de que A fuera verdadera, entonces existen hechos fuera de D que son verdaderos,
      lo que refuta la idea de que D sea completo.
    </p>
  </div>

  <!-- SECCI√ìN 5 -->
  <button type="button" class="collapsible">5) El Problema de Halting (Halting Problem)</button>
  <div class="content">
    <p>
      La segunda herramienta utilizada para demostrar la inevitabilidad de las alucinaciones estructurales
      es el Problema de Halting, uno de los m√°s fundamentales e indecidibles en la teor√≠a de la computaci√≥n.
    </p>
    <p>
      Dado un programa P y una entrada x, ¬øes posible determinar, en un tiempo finito, si P se detendr√°
      (es decir, terminar√° su ejecuci√≥n) o continuar√° ejecut√°ndose indefinidamente?
    </p>
    <p>
      Turing demostr√≥ que no existe un algoritmo general que pueda decidir, para todos los programas y entradas posibles,
      si un programa se detendr√° o no. Esto implica que los sistemas computacionales no pueden prever
      su propio comportamiento en todas las circunstancias.
    </p>
    <p>
      Los modelos de lenguaje, como sistemas computacionales complejos basados en transformadores,
      comparten la naturaleza de las m√°quinas de Turing. Esto implica que el problema de la parada
      conlleva consecuencias como:
    </p>
    <ul>
      <li><strong>Indecidibilidad del l√≠mite de generaci√≥n:</strong> Los LLMs no pueden determinar con certeza cu√°ntos tokens generar√°n en respuesta a una entrada dada.</li>
      <li><strong>Imprevisibilidad del contenido generado:</strong> Debido a la indecidibilidad del halting, los modelos no pueden prever el contenido exacto que generar√°n. Esto aumenta la probabilidad de respuestas incorrectas o alucinadas.</li>
      <li><strong>Ausencia de verificaci√≥n previa:</strong> Al no poder predecir sus propias salidas antes de generarlas, los LLMs no pueden validar la veracidad o consistencia de su respuesta antes de producirla.</li>
    </ul>
  </div>

  <!-- SECCI√ìN 6 -->
  <button type="button" class="collapsible">6) Alucinaciones y el problema de encontrar una aguja</button>
  <div class="content">
    <p>
      Este problema aborda la capacidad (o incapacidad) de los LLMs para seleccionar la informaci√≥n correcta
      dentro de un vasto conjunto de datos.
    </p>
    <p>
      El problema plantea:<br/>
      Dado un gran conjunto de datos (el pajar) y una solicitud de informaci√≥n espec√≠fica (la aguja),
      ¬øpuede un modelo de lenguaje recuperar de manera confiable y precisa esa aguja?
    </p>
    <p>
      El texto demuestra que este problema es indecidible, lo que significa que no existe un m√©todo
      que garantice que un modelo siempre recuperar√° la informaci√≥n correcta; lo cual profundiza
      el problema de las alucinaciones.
    </p>

    <!-- IMAGEN 2 -->
    <div class="separator">
      <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU18Jbsq_qf2lIFQ_LLoAMwEbOqK5Bmn-M6-otnl-Bz3oeIwc0Tj8yCDoayc1coLhCNqUzsxeJUl7JuMxzwiMPF4DQp4CbNGzSESTOycGK-lvdQ1wPeStjqlhEfJ2HT481Kk66CnS3UCW-N4BN_cmsmSGpUtKK-dFjAeE-zoJoSutOEpsMFDXAkpEQ-IA/s1384/alucinaciones%20intr%C3%ADnsecas.png"
         style="display: block; padding: 1em 0; text-align: center;"
         target="_blank" rel="noopener">
        <img alt="Alucinaciones Intr√≠nsecas" border="0" width="320"
             src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU18Jbsq_qf2lIFQ_LLoAMwEbOqK5Bmn-M6-otnl-Bz3oeIwc0Tj8yCDoayc1coLhCNqUzsxeJUl7JuMxzwiMPF4DQp4CbNGzSESTOycGK-lvdQ1wPeStjqlhEfJ2HT481Kk66CnS3UCW-N4BN_cmsmSGpUtKK-dFjAeE-zoJoSutOEpsMFDXAkpEQ-IA/s320/alucinaciones%20intr%C3%ADnsecas.png"/>
      </a>
    </div>
  </div>

  <!-- REFERENCIAS -->
  <div class="link-buttons">
    <!-- Corregimos la URL (sin doble 'h') -->
    <a href="https://arxiv.org/pdf/2409.05746" target="_blank" rel="noopener">
      LLMs Will Always Hallucinate
    </a>
  </div>

  <!-- SCRIPT PARA LOS BOTONES PLEGABLES -->
  <script>
    // Seleccionamos todos los botones con la clase "collapsible"
    const collapsibles = document.querySelectorAll(".collapsible");

    collapsibles.forEach(button => {
      button.addEventListener("click", function () {
        // Alternamos la clase "active" en el bot√≥n
        this.classList.toggle("active");
        // El siguiente elemento es el contenido plegable
        const content = this.nextElementSibling;
        if (content.style.maxHeight) {
          // Si est√° desplegado, lo plegamos
          content.style.maxHeight = null;
        } else {
          // De lo contrario, lo desplegamos
          content.style.maxHeight = content.scrollHeight + "px";
        }
      });
    });
  </script>
</body>
</html>
