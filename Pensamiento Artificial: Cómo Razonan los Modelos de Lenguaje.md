üîÅ Cambiar idioma

Pensamiento Artificial: C√≥mo Razonan los Modelos de Lenguaje


1. Introducci√≥n
En los √∫ltimos a√±os, los modelos de lenguaje han demostrado capacidades sorprendentes, desde responder preguntas con precisi√≥n hasta escribir poes√≠a o resolver acertijos. Pero una pregunta clave persiste: ¬øc√≥mo ‚Äúpiensan‚Äù estos modelos? ¬øSon simplemente predictores de la pr√≥xima palabra, o hay mecanismos internos m√°s complejos operando bajo la superficie?

Dos trabajos recientes de Anthropic nos ofrecen una nueva lente para explorar esta cuesti√≥n. El primero, Circuit Tracing, presenta un enfoque novedoso para abrir la caja negra de los modelos mediante la construcci√≥n de grafos de atribuci√≥n. El segundo, On the Biology of a Language Model, aplica esta herramienta para revelar comportamientos emergentes que van m√°s all√° de la autorregresi√≥n simple.



2. Marco conceptual: Interpretabilidad mec√°nica
La interpretabilidad mec√°nica busca entender los mecanismos internos reales que el modelo utiliza para razonar, de forma an√°loga a c√≥mo la neurociencia estudia los circuitos del cerebro.

En lugar de tratar al modelo como una caja negra, lo analiza como un sistema din√°mico compuesto por features, flujos de activaci√≥n y relaciones causales internas.

3. Del supergrafo al subgrafo: el modelo como red din√°mica
Los transformadores pueden verse como un supergrafo computacional. Pero solo una parte de ese grafo se activa por cada prompt. Esta porci√≥n activa es el grafo de atribuci√≥n, que revela c√≥mo fluye la informaci√≥n realmente.



4. Construyendo el microscopio: CLTs y modelo de reemplazo
Se introducen los Cross-Layer Transcoders (CLTs), que reemplazan las MLPs por componentes interpretables. As√≠ se construye un modelo de reemplazo que replica el comportamiento original pero permite an√°lisis internos.



5. Visualizando el razonamiento: grafos de atribuci√≥n
Con el modelo interpretado, se construyen grafos que muestran c√≥mo cada feature contribuye a una predicci√≥n. Son explicaciones lineales, cuantificables y visuales del pensamiento del modelo.



6. Usando el microscopio: an√°lisis de casos reales
Caso 1: Formaci√≥n de acr√≥nimos ‚Äì activaci√≥n de palabras clave.
Caso 2: Memoria factual ‚Äì asociaci√≥n de conceptos.
Caso 3: Sumas ‚Äì razonamiento modular.



7. ¬øLos LLMs piensan? Lo que revelan los grafos
Los modelos parecen planificar, anticipar y construir ideas estructuradas. Incluso se ajustan para mantener coherencia est√©tica (rimas) o l√≥gica (diagn√≥sticos).



8. Limitaciones actuales y futuro del campo
Cobertura parcial del modelo

Atenci√≥n no modelada completamente

Costos computacionales

Interferencia entre prompts



9. Conclusi√≥n: hacia una ciencia de la IA interpretable
Estos enfoques permiten auditar decisiones, descubrir sesgos y construir herramientas de intervenci√≥n. Los LLMs, lejos de ser cajas negras, pueden ser observados por dentro con precisi√≥n cient√≠fica.

10. Definiciones t√©cnicas
Concepto	Definici√≥n
Interpretabilidad mec√°nica	Estudio del funcionamiento interno de los modelos de IA.
Grafo de atribuci√≥n	Muestra el flujo causal entre tokens, features y logits.
Feature	Unidad lineal interpretable del modelo.
CLT	Sustituto de MLP que facilita la interpretaci√≥n.
Modelo de reemplazo	Modelo donde se sustituyen MLPs por CLTs.
Modelo congelado	Versi√≥n donde se fijan atenci√≥n y normalizaci√≥n.
Linealidad de atribuci√≥n	Propiedad que permite contribuciones aditivas entre unidades.
Supergrafo computacional	Todas las rutas de c√≥mputo posibles en el modelo.
Perturbaci√≥n e intervenci√≥n	T√©cnicas experimentales para validar relaciones causales.
11. Referencias
Circuit Tracing: https://transformer-circuits.pub/2025/attribution-graphs/methods.html

On the Biology of a Language Model: https://transformer-circuits.pub/2025/attribution-graphs/biology.html
